{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f841e84f",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a527d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5801db",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a760e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8733 files belonging to 4 classes.\n",
      "Found 2570 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 #instead of processing one image at a time, the images are processed in a batch of 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "# \"tf.keras.utils.image_dataset_from_directory\" is a library function which loads images from a directory\n",
    "# we have two directories here, test and train, each directory here has four classes\n",
    "# label_mode defines how the lables are represented by the library function \n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"D:\\Study stuff\\Fruit insights project\\model\\Model-1-fruit-freshness\\train\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',  \n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"D:\\Study stuff\\Fruit insights project\\model\\Model-1-fruit-freshness\\test\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',  \n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c13b0",
   "metadata": {},
   "source": [
    "### Normalizing pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are normalizing pixel values because if data is collected from multiple resources and all images have different lighting conditions, it helps to standardize the input \n",
    "\n",
    "# we are creating a normalization layer, it rescales the input images by multiplying each image by 1/255. The original pixel values range from 0 to 255, and by this multiplication we convert them to floating point values between 0.0 and 1.0\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# here the map() applies the lambda function to each batch\n",
    "# x is a batch of images\n",
    "# y is a batch of labels for each image in the batch\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce36cd",
   "metadata": {},
   "source": [
    "### Optimizing data set loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2763ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.AUTOTUNE makes sure the data pipeline has the right number of threads working in parallel so training is smooth and efficient.\n",
    "\n",
    "# data pipeline includes a number of processes such as, loading them from the disk, resizing them, shuffling them, normalizing them, batch them and feed them to the model.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# prefetch() prepares the next batch of data while the model is being trained on the current one\n",
    "# shuffle() is used because it randomizes the order of images so that the machine is not focusing on a specific order during training\n",
    "# cache() places the data in cache memory because it speeds up the training process by avoiding the need to read data from disk repeatedly\n",
    "# prefetch(buffer_size=AUTOTUNE) decides how many batches of prefetched data should be prepared ahead of time (in RAM) to keep the training pipeline smooth.\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08939b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
