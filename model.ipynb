{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f841e84f",
   "metadata": {},
   "source": [
    "# A basic ML model to check the freshness of a fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d39dc62",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a527d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5801db",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e007ce9",
   "metadata": {},
   "source": [
    "### Setting Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf6312",
   "metadata": {},
   "source": [
    "- **\"tf.keras.utils.image_dataset_from_directory\"** is a library function which loads images from a directory\n",
    "- we have three directories here, *test, train and validation*, each directory here has four classes\n",
    "- label_mode defines how the lables are represented by the library function\n",
    "- seed ensures the random split between training and validation dataset is consistent and no images appear in both subsets if you run your code multiple times. (since we are splitting training set into two parts instead of creating a different directory for validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a760e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8733 files belonging to 4 classes.\n",
      "Using 6987 files for training.\n",
      "Found 8733 files belonging to 4 classes.\n",
      "Using 1746 files for validation.\n",
      "Found 2570 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 #instead of processing one image at a time, the images are processed in a batch of 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "validation_split = 0.2\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"D:\\Study stuff\\Fruit insights project\\model\\Model-1-fruit-freshness\\train\",\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',  \n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"D:\\Study stuff\\Fruit insights project\\model\\Model-1-fruit-freshness\\train\",\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"D:\\Study stuff\\Fruit insights project\\model\\Model-1-fruit-freshness\\test\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',  \n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c13b0",
   "metadata": {},
   "source": [
    "### Normalizing pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c35e0",
   "metadata": {},
   "source": [
    "- Raw images have pixel values in range 0-255\n",
    "- The neural networks train faster in a standardized range (0.0 - 1.0). For that we use normalization, it reduces effects of variations in lightning, brightness and contrast in images.\n",
    "- **normalization_layer** rescales each pixel by multiplying it from (1/255) which converts the range of pixels to a floating point number between 0.0 to 1.0.\n",
    "\n",
    "---\n",
    "\n",
    "- here the **map()** applies the lambda function to each batch\n",
    "- **x** is a batch of images\n",
    "- **y** is a batch of labels for each image in the batch (categories of images i.e. fresh apples, fresh oranges etc)\n",
    "- you can imagine labels are [0,1,2,3] where 0 is for fresh apples, 1 is for fresh oranges, 2 is for rotten apples and 3 is for rotten oranges. The labels remain the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedc29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce36cd",
   "metadata": {},
   "source": [
    "### Optimizing data set loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76899d05",
   "metadata": {},
   "source": [
    "- **tf.data.AUTOTUNE** makes sure the data pipeline has the right number of threads working in parallel so training is smooth and efficient. Here threads are for the number of operations like map(), prefetch() etc. that are present in the data pipeline.\n",
    "- **data pipeline** includes a number of processes such as, loading them from the disk, resizing them, shuffling them, normalizing them, batch them and feed them to the model.\n",
    "- **cache()**: Stores the data in RAM after the first epoch (or on disk if RAM is small), this avoids reloading images again in later epochs which results in faster training.\n",
    "- **shuffle():** is used because it randomizes the order of images so that the machine is not focusing on a specific order during training, tensorflow randomly selects the next image from the buffer.\n",
    "- **prefetch()**: prepares the next batch of data while the model is being trained on the current one\n",
    "- **prefetch(buffer_size=AUTOTUNE)**: lets tensorflow automatically choose the optimal number of batches to prepare in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2763ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23af03",
   "metadata": {},
   "source": [
    "## CNN model\n",
    "( Convolutional Neural Network )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c66ed",
   "metadata": {},
   "source": [
    "- The **layers module** provides building blocks of a neural network. Here each layer performs a specific transformation on data.\n",
    "Such as - **Conv2D** : detects patterns/features\n",
    "        - **MaxPooling2D** : shrinks data while keeping important information\n",
    "        - **Flatten** : Converts Multidimentional data into 1D vector\n",
    "        - **Dense** : Combines all features to make a decision\n",
    "\n",
    "- The **models module** provides a container to organize and connect the layers together into a complete neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08939b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e0387",
   "metadata": {},
   "source": [
    "### Model creation\n",
    "\n",
    "I am using a **Sequential model**, which means the output of one layer is directly passed as input to the next (layers are stacked one after another). The steps are given below. \n",
    "\n",
    "#### 1. Input Layer\n",
    "- The model takes an image of size **180 × 180** with **3 color channels (RGB)**.\n",
    "\n",
    "#### 2. Convolution + Pooling Layers (Feature Extraction)\n",
    "- **Conv2D(32, 3×3, ReLU):** Detects very basic features like *edges, corners, and lines*. \n",
    "- **ReLU:** ensures we only keep the useful signals by removing negative values\n",
    "- **MaxPooling(2×2):** Shrinks the image while keeping the strongest signals. This process makes the model faster and avoids overfitting.\n",
    "- **Dropout(0.25):** Dropout randomly sets 25% of those values to 0. This prevents the model to memorize which helps to focus on learning different patterns.  \n",
    "\n",
    "- **Conv2D(64, 3×3, ReLU):** Detects more complex features like *textures, curves, and shapes*.  \n",
    "- **MaxPooling(2×2):** Same function\n",
    "- **Dropout(0.25):** Same function\n",
    "\n",
    "- **Conv2D(128, 3×3, ReLU):** Detects high-level, detailed features like *eyes, leaves, or object parts*.  \n",
    "- **MaxPooling(2×2):** Same function. \n",
    "- **Dropout(0.25):** Same function\n",
    "\n",
    "At this stage, the image has been turned into a set of **2D feature maps** (many small grids that represent different learned patterns).\n",
    "\n",
    "#### 3. Flatten Layer\n",
    "- The 2D feature maps are **flattened into a 1D vector**.  \n",
    "- This means all the detected features are lined up in a row so they can be used for decision making.\n",
    "\n",
    "#### 4. Dense(128, ReLU)\n",
    "- This is a **fully connected layer** that learns **combinations of features**.  \n",
    "- Example: *“If this edge + this texture + this color appear together  :- it might mean ‘fresh fruit’.”*  \n",
    "- **Dropout(0.50):** In dense layers every neuron is connected to every neuron in next layer so it makes them prone to *overfitting* (memorizing), therefore, dropping 50% of neurons ensures that model focuses on general patterns instead of memorizing.\n",
    "\n",
    "#### 5. Dense(4, Softmax)\n",
    "- This layers has 4 neurons (because we have 4 classes). So each neuron corresponds to one class.\n",
    "- The output comes out as raw values called logits, they can be positive or negative.\n",
    "- Softmax converts them into probabilities that add up to 1. \n",
    "- For example if logits are [-0.35, 0.1, 3.4, 0.01]\n",
    "- then after softmax it will be [0.8, 0.1, 0.05, 0.05] (sum of them is equal to 1). This means that the machine is 80% confident that the given image belongs to first class.\n",
    "\n",
    "In conclusion, Convolution + pooling are feature extracters, the dense layers act as decision makers, softmax gives us probabilites for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b254c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">178</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">178</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51200</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,553,728</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m178\u001b[0m, \u001b[38;5;34m178\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m87\u001b[0m, \u001b[38;5;34m87\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51200\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m6,553,728\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,647,492</span> (25.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,647,492\u001b[0m (25.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,647,492</span> (25.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,647,492\u001b[0m (25.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Input(shape=(180, 180, 3)),\n",
    "\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(4, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec17ff",
   "metadata": {},
   "source": [
    "### Compilation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2687dd9",
   "metadata": {},
   "source": [
    "- Here the neural network gets ready for training\n",
    "- **optimizer** is like an engine of learning.\n",
    "- **'adam'** (Adaptive moment estimation) automatically updates the learning rate for each parameter so the model continues to learn from it's mistakes.\n",
    "- The **loss function** measures how far the model's predictions are from the actual data.\n",
    "- **Categorical crossentropy** focuses on the correct class and checks how confident the model is about predicting it.\n",
    "- **metrics** tell us how well the model is performing, accuracy simply means the percentage of correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78fc984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcb1fd",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ab3e9",
   "metadata": {},
   "source": [
    "- **EarlyStopping** is used to prevent overfitting. It will watch the validation loss and stops training when there is no improvement.\n",
    "- **patience = 2** means it will wait for 2 epochs to see if the validation loss improves and if it doesn't then it will stop the training.\n",
    "- **restore_best_weights=True** makes sure the best version of the model is kept\n",
    "---\n",
    "- **epochs** means that the model will go through the entire training dataset 10 times and each time it will try to improve.\n",
    "---\n",
    "- **model.fit** trains the neural network, it takes the training dataset in batches and feeds it to the model.\n",
    "- After each epoch, the model will check its perfomance on validation data to see if it's learning patterns or overfitting(memorizing).\n",
    "- Normally it will run for 10 epochs. Where first epoch will be 1/10 and last will be 10/10.\n",
    "- But with early_stop the training may stop before 10 epochs if the validation loss is not improving.\n",
    "- This training process is saved in \"history\"\n",
    "---\n",
    "The training process is tracked with:\n",
    "- accuracy → how well it predicts on training data\n",
    "- loss → error on training data\n",
    "- val_accuracy → how well it predicts on validation data\n",
    "- val_loss → error on validation data\n",
    "---\n",
    "***Why are we checking validation loss?***\n",
    "The model sees training data again and again so it might memorize it. Validation data is the unseen data so with that data we check if our model is learning or memorizing. By improvement in validation loss value, we mean the lower it gets the better. When the validation loss starts going up, that's when the earlyStopping kicks in. If the validation loss stops improving it means that the model is memorizing training data and it's not generalizing well.\n",
    "\n",
    "***Accuracy vs loss***\n",
    "- Accuracy is a percentage of correctly predicted labels.\n",
    "- Loss is a numerical score which tells how wrong the predictions are. \n",
    "- We are using *categorical crossentrophy* which looks at the predicted probabilities (from softmax). The higher the probability for correct class, smaller the loss. vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae90c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 98/219\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3:30\u001b[0m 2s/step - accuracy: 0.3936 - loss: 1.6507"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience = 2, restore_best_weights=True)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,  \n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6625c",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2858d1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# collecting the data we received from model training for each epoch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m acc = \u001b[43mhistory\u001b[49m.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m val_acc = history.history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m loss = history.history[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# collecting the data we received from model training for each epoch\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs) # used as x-axis values\n",
    "\n",
    "plt.figure(figsize=(12, 6)) # figure size , 12 width and 6 height\n",
    "plt.subplot(1, 2, 1) # 1 row, 2 columns, index (first subplot)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2) # 1 row, 2 columns, index (second subplot)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98919c6",
   "metadata": {},
   "source": [
    "> So from the above plots let's understand what we got.\n",
    "- **Training and Validation accuracy**: Both curves are steadily increasing and validation accuracy follows training accuracy closely which means the model is generalizing well (not overfitting). The accuracy reaches close to 95%.\n",
    "- **Training and Validation loss**: Both losses are decreasing nicely. Since validation loss continues to go down (instead of rising up in the end), it means that the model is effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca4267",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176a06a",
   "metadata": {},
   "source": [
    "- **model.evaluate(test_ds)** : It runs the model in test dataset using the same metrics we defined during compilation (loss and accuracy). It returns them as a list [loss, accuracy]\n",
    "- **test_loss** : error score on test dataset\n",
    "- **test_acc** : percentage of correct predictions on test dataset\n",
    "- **print(f\"Test accuracy: {test_acc:.3f}\")** : prints accuracy rounded to 3 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012e7ad9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_loss, test_acc = \u001b[43mmodel\u001b[49m.evaluate(test_ds)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23662b7",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step\n",
      "Predicted class (first 5): [0 0 0 0 0]\n",
      "True class (first 5): [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_ds.take(1):  # Takes one batch from test data\n",
    "    predictions = model.predict(images)\n",
    "    print(\"Predicted class (first 5):\", predictions.argmax(axis=1)[:5])\n",
    "    print(\"True class (first 5):\", labels.numpy().argmax(axis=1)[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8807c13",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ed05519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('fruit_freshness_cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36879d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
